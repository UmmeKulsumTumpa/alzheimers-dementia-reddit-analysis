{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38dnPZ3bCp75"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict\n",
    "# ========= user settings =========\n",
    "INPUT_CSV_1  = \"/inputPath/FileName\"\n",
    "INPUT_CSV_2  = \"/inputPath/FileName\"\n",
    "OUTPUT_CSV_1 = \"/outputPath/FileName\"\n",
    "OUTPUT_CSV_2 = \"/outputPath/FileName\"\n",
    "\n",
    "CHUNK_SIZE = 1000          # rows per pandas chunk read\n",
    "SAVE_EVERY_ROWS = 200      # flush to disk at least this many new rows\n",
    "OUTPUT_COL = \"author_category\"  # name of the new column\n",
    "OUTPUT_REL = \"author_relationship\"  # name of the new column\n",
    "# =================================\n",
    "\n",
    "# You provide these:\n",
    "# from my_pipeline import build_prompt, get_llm_output\n",
    "\n",
    "def infer_categorization(model, tokenizer, prompt: str, max_new_tokens=5) -> str:\n",
    "\n",
    "    inputs = tokenizer(prompt, padding=False, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_new_tokens=1,     # <- prevents immediate empty decode\n",
    "        do_sample=not False,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    gen_ids = outputs.sequences[0]\n",
    "    input_len = inputs[\"input_ids\"].shape[1] if gen_ids.shape[0] > inputs[\"input_ids\"].shape[1] else 0\n",
    "    gen_only_ids = gen_ids[input_len:]\n",
    "    response = tokenizer.decode(gen_only_ids, skip_special_tokens=True)#[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def load_model_tokenizer(model_checkpoint, tuned = False):\n",
    "\n",
    "    if not tuned:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\":0},\n",
    "        )\n",
    "\n",
    "        tokenizer= AutoTokenizer.from_pretrained(model_checkpoint, padding_side = 'left')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(model_checkpoint=\"Qwen/Qwen2.5-14B-Instruct\")\n",
    "\n",
    "\n",
    "def generate_prompt(post_text):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert annotator of posts from health subreddits (e.g., r/Alzheimers, r/dementia).\n",
    "Classify the author's perspective into EXACTLY ONE category using ONLY the post content—no outside knowledge or assumptions.\n",
    "\n",
    "CATEGORIES\n",
    "A = \"Caregiver about patient\" — Caregiver mainly describes the patient’s symptoms/behaviors/care decisions.\n",
    "B = \"Caregiver about themselves\" — Caregiver mainly describes their own emotions/burden/logistics (not the patient’s medical details).\n",
    "C = \"Patient about themselves\" — Author states or clearly implies they are diagnosed and describes their own condition.\n",
    "D = \"Non-diagnosed patient about themselves\" — Author worries they might have the condition, describes own symptoms, but NO diagnosis mentioned.\n",
    "E = \"Other\" — Not classifiable into A–D, off-topic, or insufficient evidence.\n",
    "\n",
    "DECISION RULES\n",
    "1) If the author explicitly states a diagnosis for themselves (e.g., “I was diagnosed…”, “My neurologist confirmed…”), choose C.\n",
    "2) Else if the author focuses on their own symptoms/fears without a diagnosis, choose D.\n",
    "3) Else if the author is a caregiver and the primary focus is the patient’s condition/care, choose A.\n",
    "4) Else if the author is a caregiver and the primary focus is the caregiver’s feelings/stress/logistics, choose B.\n",
    "5) If multiple apply, choose the ONE category that covers >50% of the post by content/focus. If still ambiguous, choose E.\n",
    "6) Ignore quoted/reposted material unless the author clearly endorses it as their own experience.\n",
    "\n",
    "RELATIONSHIP EXTRACTION\n",
    "Additionally, identify the author's relationship to the person with Alzheimer’s or dementia (if any). Use the most specific term evident from the post (e.g., \"spouse\", \"child\", \"parent\", \"friend\", \"professional caregiver\"). If the post is about the author themselves (C or D), use \"self\". If no relationship is mentioned or discernible, use \"unspecified\".\n",
    "\n",
    "OUTPUT (STRICT JSON ONLY — no extra text):\n",
    "{{\n",
    "  \"category\": \"A\" | \"B\" | \"C\" | \"D\" | \"E\",\n",
    "  \"relationship\": \"<string>\"\n",
    "}}\n",
    "\n",
    "Post:\n",
    "'''{post_text}'''\n",
    "\"\"\"\n",
    "    return prompt\n",
    "# PROMPT_STRICT = (\n",
    "#     PROMPT_ORIGINAL\n",
    "# )\n",
    "\n",
    "\n",
    "VALID_CATS = {\"A\", \"B\", \"C\", \"D\", \"E\"}\n",
    "CAT_MAP = {\n",
    "    \"A\": \"Caregiver about patient\",\n",
    "    \"B\": \"Caregiver about themselves\",\n",
    "    \"C\": \"Patient about themselves\",\n",
    "    \"D\": \"Non-diagnosed patient about themselves\",\n",
    "    \"E\": \"Other\"\n",
    "}\n",
    "\n",
    "def _first_fenced_block(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return the first fenced code block content if present, else None.\n",
    "    Accepts ```json ...``` or ``` ...```.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def _first_json_object(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Best-effort scan to extract the first top-level {...} block.\n",
    "    Returns the substring including braces, or None.\n",
    "    \"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    while start != -1:\n",
    "        depth = 0\n",
    "        for i in range(start, len(text)):\n",
    "            ch = text[i]\n",
    "            if ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    return text[start:i+1]\n",
    "        # no balanced close; look for next '{'\n",
    "        start = text.find(\"{\", start + 1)\n",
    "    return None\n",
    "\n",
    "def parse_category_relationship(llm_out: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse model output to {'category': <A|B|C|D|E>, 'relationship': <string>}.\n",
    "    Returns None on failure (no exceptions).\n",
    "    \"\"\"\n",
    "    candidate = _first_fenced_block(llm_out) or _first_json_object(llm_out)\n",
    "    if not candidate:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(candidate)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if not isinstance(obj, dict):\n",
    "        return None\n",
    "\n",
    "    # print(\"HERE\")\n",
    "    cat = obj.get(\"category\")\n",
    "    rel = obj.get(\"relationship\")\n",
    "\n",
    "    if not isinstance(cat, str) or cat not in VALID_CATS:\n",
    "        cat = CAT_MAP.get(\"E\")#return None\n",
    "\n",
    "    if not isinstance(rel, str) or not rel.strip():\n",
    "        rel = \"unspecified\"  # normalize empty/missing to \"unspecified\"\n",
    "\n",
    "    # Return exactly the two required keys\n",
    "    return {\"category\": CAT_MAP.get(cat, \"\"), \"relationship\": rel.strip()}\n",
    "\n",
    "def load_processed_ids(out_path: Path, id_col: str) -> set:\n",
    "    \"\"\"\n",
    "    Load IDs already written to OUTPUT_CSV so we can skip them.\n",
    "    Reads in chunks to avoid high memory use.\n",
    "    \"\"\"\n",
    "    processed = set()\n",
    "    if not out_path.exists():\n",
    "        return processed\n",
    "\n",
    "    for chunk in pd.read_csv(out_path, usecols=[id_col], dtype={id_col: \"string\"}, chunksize=200_000):\n",
    "        processed.update(chunk[id_col].dropna().astype(\"string\"))\n",
    "    return processed\n",
    "\n",
    "def append_buffer_to_csv(buffer_rows, out_path: Path):\n",
    "    \"\"\"Append buffered rows to CSV (create header once).\"\"\"\n",
    "    if not buffer_rows:\n",
    "        return\n",
    "    df = pd.DataFrame(buffer_rows)\n",
    "    write_header = not out_path.exists()\n",
    "    df.to_csv(out_path, mode=\"a\", header=write_header, index=False)\n",
    "    buffer_rows.clear()\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Stream the input in chunks so we don't load the whole file in memory.\n",
    "        for INPUT_CSV, OUTPUT_CSV in [(INPUT_CSV_1, OUTPUT_CSV_1), (INPUT_CSV_2, OUTPUT_CSV_2)]:\n",
    "            out_path = Path(OUTPUT_CSV)\n",
    "            processed_so_far = load_processed_ids(out_path, id_col=\"id\")\n",
    "            print(f\"[resume] Already processed rows: {processed_so_far}\")\n",
    "\n",
    "            buffer_rows = []\n",
    "            for chunk in tqdm(pd.read_csv(INPUT_CSV, chunksize=CHUNK_SIZE), desc=\"Processing Chunks\"):\n",
    "                for row in tqdm(chunk.itertuples(index=False), desc=\"Processing Rows\"):\n",
    "                    # Access the selftext column\n",
    "                    post_id = getattr(row, \"id\", \"\")\n",
    "                    if post_id in processed_so_far:\n",
    "                        # skip already done\n",
    "                        continue\n",
    "                    selftext = getattr(row, \"selftext\", \"\")\n",
    "                    if pd.isna(selftext):\n",
    "                        selftext = \"\"\n",
    "\n",
    "                    # Build your prompt\n",
    "                    prompt = generate_prompt(post_text=selftext)\n",
    "                    # Run your LLM\n",
    "                    llm_out = infer_categorization(model, tokenizer, prompt, max_new_tokens=18)#_safe_llm_call(selftext)\n",
    "                    # print(f'prompt : {prompt}')\n",
    "                    # print(f\"[{post_id}] : {llm_out}\")\n",
    "\n",
    "                    extracted_json = parse_category_relationship(llm_out)\n",
    "                    # print(f'extracted_json : {extracted_json}')\n",
    "                    # Copy the entire row + the new column\n",
    "                    rec = row._asdict()  # dict of all columns in the row\n",
    "                    rec[OUTPUT_COL] = extracted_json[\"category\"] if extracted_json else CAT_MAP.get(\"E\")#\"Other\"\n",
    "                    rec[OUTPUT_REL] = extracted_json[\"relationship\"] if extracted_json else \"unspecified\"\n",
    "                    buffer_rows.append(rec)\n",
    "\n",
    "                    # Periodic save (row-based or time-based)\n",
    "                    if (len(buffer_rows) >= SAVE_EVERY_ROWS):\n",
    "                        append_buffer_to_csv(buffer_rows, out_path)\n",
    "                    break\n",
    "                break\n",
    "            # Final flush\n",
    "            append_buffer_to_csv(buffer_rows, out_path)\n",
    "\n",
    "    finally:\n",
    "        # Ensure any remaining rows get saved even if interrupted\n",
    "        append_buffer_to_csv(buffer_rows, out_path)\n",
    "        print(\"[done] Saved all buffered rows.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
