{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_r0u75ubEYS",
        "outputId": "cc56e727-ef7b-4cce-8150-ba1dedb679b4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsUlRvXb-Ab",
        "outputId": "ab518ed5-cdb5-440a-cfa0-40282fed4d55"
      },
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = ['tqdm', 'pandas', 'requests']\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "print(\"‚úÖ Setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueb-ObG4cWpI"
      },
      "source": [
        "# Confguration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkakbEIXcAM9",
        "outputId": "4869cf0f-30a8-44a4-f185-30615c00b719"
      },
      "outputs": [],
      "source": [
        "# Folder for data storage (replace with your path or mount in Colab)\n",
        "DRIVE_FOLDER = '/inputPath/FileName'\n",
        "\n",
        "# Subreddits to scrape (modify as needed)\n",
        "SUBREDDITS = ['alzheimers', 'dementia']\n",
        "\n",
        "# Date range (15-16 years as requested)\n",
        "START_DATE = '2010-01-01'\n",
        "END_DATE = '2025-07-20'\n",
        "\n",
        "# Batch sizes (adjust based on your needs)\n",
        "POST_BATCH_SIZE = 1000\n",
        "COMMENT_BATCH_SIZE = 2000\n",
        "\n",
        "print(f\"üìÅ Data will be saved to: {DRIVE_FOLDER}\")\n",
        "print(f\"üéØ Target subreddits: {SUBREDDITS}\")\n",
        "print(f\"üìÖ Date range: {START_DATE} to {END_DATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxGClT9Gcfrd"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = 'https://arctic-shift.photon-reddit.com/api'\n",
        "HEADERS = {'User-Agent': 'RedditResearchScraper/1.0'}\n",
        "REQUEST_DELAY = 1\n",
        "MAX_RETRIES = 5\n",
        "RETRY_DELAY = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kNzkRUIc84Y"
      },
      "outputs": [],
      "source": [
        "class ColabRedditScraper:\n",
        "    def __init__(self, drive_folder=DRIVE_FOLDER):\n",
        "        self.drive_folder = drive_folder\n",
        "        os.makedirs(drive_folder, exist_ok=True)\n",
        "        print(f\"üìÅ Created/verified folder: {drive_folder}\")\n",
        "\n",
        "    def make_request(self, endpoint, params, max_retries=MAX_RETRIES):\n",
        "        \"\"\"Make API request with retry logic\"\"\"\n",
        "        url = f\"{BASE_URL}/{endpoint}\"\n",
        "        retries = 0\n",
        "\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                response = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    # Check rate limits\n",
        "                    remaining = response.headers.get('X-RateLimit-Remaining')\n",
        "                    if remaining and int(remaining) < 5:\n",
        "                        reset_time = response.headers.get('X-RateLimit-Reset')\n",
        "                        if reset_time:\n",
        "                            wait_time = max(1, int(reset_time) - int(time.time()))\n",
        "                            print(f\"‚ö†Ô∏è Rate limit low. Waiting {wait_time}s...\")\n",
        "                            time.sleep(wait_time)\n",
        "\n",
        "                    return response.json()\n",
        "\n",
        "                elif response.status_code == 429:\n",
        "                    print(\"‚ö†Ô∏è Rate limited. Waiting 60 seconds...\")\n",
        "                    time.sleep(60)\n",
        "\n",
        "                elif response.status_code == 525:\n",
        "                    print(f\"‚ö†Ô∏è Cloudflare error. Retrying in {RETRY_DELAY}s...\")\n",
        "                    time.sleep(RETRY_DELAY)\n",
        "\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è HTTP {response.status_code}: {response.text[:200]}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Request error: {e}\")\n",
        "\n",
        "            retries += 1\n",
        "            wait_time = min(300, 2 ** retries)\n",
        "            print(f\"üîÅ Retry {retries}/{max_retries} in {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_checkpoint(self, subreddit, data_type, last_timestamp, batch_num, total_count):\n",
        "        \"\"\"Save checkpoint for resuming\"\"\"\n",
        "        checkpoint = {\n",
        "            'subreddit': subreddit,\n",
        "            'data_type': data_type,\n",
        "            'last_timestamp': last_timestamp,\n",
        "            'batch_num': batch_num,\n",
        "            'total_count': total_count,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        checkpoint_file = os.path.join(self.drive_folder, f\"checkpoint_{subreddit}_{data_type}.json\")\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "        print(f\"üíæ Saved checkpoint: {subreddit} {data_type} - {total_count} items\")\n",
        "\n",
        "    def load_checkpoint(self, subreddit, data_type):\n",
        "        \"\"\"Load checkpoint for resuming\"\"\"\n",
        "        checkpoint_file = os.path.join(self.drive_folder, f\"checkpoint_{subreddit}_{data_type}.json\")\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'r') as f:\n",
        "                checkpoint = json.load(f)\n",
        "            print(f\"üìÇ Resuming from checkpoint: {checkpoint['total_count']} {data_type}\")\n",
        "            return checkpoint\n",
        "        return None\n",
        "\n",
        "    def scrape_posts(self, subreddit, resume=True):\n",
        "        \"\"\"Scrape all posts from subreddit\"\"\"\n",
        "        print(f\"\\nüìù Starting post scraping for r/{subreddit}\")\n",
        "\n",
        "        # Check for existing checkpoint\n",
        "        checkpoint = None\n",
        "        if resume:\n",
        "            checkpoint = self.load_checkpoint(subreddit, 'posts')\n",
        "\n",
        "        # Set up parameters\n",
        "        params = {\n",
        "            'subreddit': subreddit,\n",
        "            'after': checkpoint['last_timestamp'] if checkpoint else START_DATE,\n",
        "            'before': END_DATE,\n",
        "            'limit': 100,\n",
        "            'sort': 'asc',\n",
        "            'fields': 'id,title,selftext,author,created_utc,score,num_comments,url,over_18'\n",
        "        }\n",
        "\n",
        "        batch_num = checkpoint['batch_num'] if checkpoint else 1\n",
        "        total_count = checkpoint['total_count'] if checkpoint else 0\n",
        "        current_batch = []\n",
        "\n",
        "        progress_bar = tqdm(desc=f\"r/{subreddit} posts\", initial=total_count)\n",
        "\n",
        "        while True:\n",
        "            data = self.make_request('posts/search', params)\n",
        "            if not data:\n",
        "                print(\"‚ùå Failed to fetch data\")\n",
        "                break\n",
        "\n",
        "            posts = data.get('data', [])\n",
        "            if not posts:\n",
        "                print(\"‚úÖ No more posts found\")\n",
        "                break\n",
        "\n",
        "            current_batch.extend(posts)\n",
        "            total_count += len(posts)\n",
        "            progress_bar.update(len(posts))\n",
        "\n",
        "            # Save batch when it reaches the limit\n",
        "            if len(current_batch) >= POST_BATCH_SIZE:\n",
        "                self.save_batch(current_batch, f\"{subreddit}_posts_batch_{batch_num}\")\n",
        "                self.save_checkpoint(subreddit, 'posts', posts[-1]['created_utc'], batch_num + 1, total_count)\n",
        "                current_batch = []\n",
        "                batch_num += 1\n",
        "\n",
        "            # Update pagination\n",
        "            params['after'] = posts[-1]['created_utc']\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "\n",
        "        # Save remaining posts\n",
        "        if current_batch:\n",
        "            self.save_batch(current_batch, f\"{subreddit}_posts_batch_{batch_num}\")\n",
        "\n",
        "        progress_bar.close()\n",
        "        print(f\"‚úÖ Completed posts for r/{subreddit}: {total_count:,} posts\")\n",
        "        return total_count\n",
        "\n",
        "    def scrape_comments(self, subreddit, resume=True):\n",
        "        \"\"\"Scrape all comments from subreddit\"\"\"\n",
        "        print(f\"\\nüí¨ Starting comment scraping for r/{subreddit}\")\n",
        "\n",
        "        # Check for existing checkpoint\n",
        "        checkpoint = None\n",
        "        if resume:\n",
        "            checkpoint = self.load_checkpoint(subreddit, 'comments')\n",
        "\n",
        "        # Set up parameters\n",
        "        params = {\n",
        "            'subreddit': subreddit,\n",
        "            'after': checkpoint['last_timestamp'] if checkpoint else START_DATE,\n",
        "            'before': END_DATE,\n",
        "            'limit': 100,\n",
        "            'sort': 'asc',\n",
        "            'fields': 'id,body,author,link_id,parent_id,created_utc,score'\n",
        "        }\n",
        "\n",
        "        batch_num = checkpoint['batch_num'] if checkpoint else 1\n",
        "        total_count = checkpoint['total_count'] if checkpoint else 0\n",
        "        current_batch = []\n",
        "\n",
        "        progress_bar = tqdm(desc=f\"r/{subreddit} comments\", initial=total_count)\n",
        "\n",
        "        while True:\n",
        "            data = self.make_request('comments/search', params)\n",
        "            if not data:\n",
        "                print(\"‚ùå Failed to fetch data\")\n",
        "                break\n",
        "\n",
        "            comments = data.get('data', [])\n",
        "            if not comments:\n",
        "                print(\"‚úÖ No more comments found\")\n",
        "                break\n",
        "\n",
        "            current_batch.extend(comments)\n",
        "            total_count += len(comments)\n",
        "            progress_bar.update(len(comments))\n",
        "\n",
        "            # Save batch when it reaches the limit\n",
        "            if len(current_batch) >= COMMENT_BATCH_SIZE:\n",
        "                self.save_batch(current_batch, f\"{subreddit}_comments_batch_{batch_num}\")\n",
        "                self.save_checkpoint(subreddit, 'comments', comments[-1]['created_utc'], batch_num + 1, total_count)\n",
        "                current_batch = []\n",
        "                batch_num += 1\n",
        "\n",
        "            # Update pagination\n",
        "            params['after'] = comments[-1]['created_utc']\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "\n",
        "        # Save remaining comments\n",
        "        if current_batch:\n",
        "            self.save_batch(current_batch, f\"{subreddit}_comments_batch_{batch_num}\")\n",
        "\n",
        "        progress_bar.close()\n",
        "        print(f\"‚úÖ Completed comments for r/{subreddit}: {total_count:,} comments\")\n",
        "        return total_count\n",
        "\n",
        "    def save_batch(self, data, filename):\n",
        "        \"\"\"Save batch to Drive\"\"\"\n",
        "        filepath = os.path.join(self.drive_folder, f\"{filename}.json\")\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(data, f, indent=2, default=str)\n",
        "        print(f\"üíæ Saved {len(data)} items to {filename}.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8FQtUvQdMg2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_complete_scraping():\n",
        "    \"\"\"Run the complete scraping process\"\"\"\n",
        "    scraper = ColabRedditScraper()\n",
        "\n",
        "    total_stats = {\n",
        "        'start_time': datetime.now().isoformat(),\n",
        "        'subreddits': {},\n",
        "        'total_posts': 0,\n",
        "        'total_comments': 0\n",
        "    }\n",
        "\n",
        "    for subreddit in SUBREDDITS:\n",
        "        print(f\"\\nüéØ Processing r/{subreddit}\")\n",
        "\n",
        "        try:\n",
        "            # Scrape posts\n",
        "            posts_count = scraper.scrape_posts(subreddit)\n",
        "\n",
        "            # Scrape comments\n",
        "            comments_count = scraper.scrape_comments(subreddit)\n",
        "\n",
        "            # Update stats\n",
        "            total_stats['subreddits'][subreddit] = {\n",
        "                'posts': posts_count,\n",
        "                'comments': comments_count\n",
        "            }\n",
        "            total_stats['total_posts'] += posts_count\n",
        "            total_stats['total_comments'] += comments_count\n",
        "\n",
        "            print(f\"üéâ Completed r/{subreddit}: {posts_count:,} posts, {comments_count:,} comments\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"‚ö†Ô∏è Scraping interrupted by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing r/{subreddit}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Save final summary\n",
        "    total_stats['end_time'] = datetime.now().isoformat()\n",
        "    summary_file = os.path.join(scraper.drive_folder, 'final_summary.json')\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(total_stats, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"\\nüéâ SCRAPING COMPLETED!\")\n",
        "    print(f\"üìä Total Posts: {total_stats['total_posts']:,}\")\n",
        "    print(f\"üìä Total Comments: {total_stats['total_comments']:,}\")\n",
        "    print(f\"üìÅ Data saved to: {scraper.drive_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu15_T2kdOO3",
        "outputId": "cbf5116e-ed7a-4b03-92f3-f57e614198f8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting Reddit Data Scraping for Research\")\n",
        "    print(\"=\" * 50)\n",
        "    run_complete_scraping()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
